{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Customer Reviews Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge:\n",
    "Develop a robust Sentiment Analysis classifier for XYZ customer reviews, automating the categorization into positive, negative, or neutral sentiments. Utilize Natural Language Processing (NLP) techniques, exploring different sentiment analysis methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "XYZ organization, a global online retail giant, accumulates a vast number of customer reviews daily. Extracting sentiments from these reviews offers insights into customer satisfaction, product quality, and market trends. The challenge is to create an effective sentiment analysis model that accurately classifies XYZ customer reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Instructions:\n",
    "\n",
    "1. Make sure this ipynb file that you have cloned is in the __Project__ folder on the Desktop. The Dataset is also available in the same folder.\n",
    "2. Ensure that all the cells in the notebook can be executed without any errors.\n",
    "3. Once the Challenge has been completed, save the SentimentAnalysis.ipynb notebook in the __*Project*__ Folder on the desktop. If the file is not present in that folder, autoevalution will fail.\n",
    "4. Print the evaluation metrics of the model. \n",
    "5. Before you submit the challenge for evaluation, please make sure you have assigned the Accuracy score of the model that was created for evaluation.\n",
    "6. Assign the Accuracy score obtained for the model created in this challenge to the specified variable in the predefined function *submit_accuracy_score*. The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
    "7. Please do not make any changes to the variable names and the function name *submit_accuracy_score* as this will be used for automated evaluation of the challenge. Any modification in these names will result in unexpected behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------- CHALLENGE CODE STARTS HERE --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/labuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import nltk\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/labuser/Desktop/Project/Reviews.csv', nrows=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the column name containing sentiment labels is 'Score'\n",
    "label_column = 'Score'\n",
    " \n",
    "# Apply label encoding to the actual label column\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df[label_column])\n",
    " \n",
    "# Text preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha()]  # Lemmatization\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text',\n",
      "       'encoded_label'],\n",
      "      dtype='object')\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \\\n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "   encoded_label  \n",
      "0              4  \n",
      "1              0  \n",
      "2              3  \n",
      "3              1  \n",
      "4              4  \n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Check column names\n",
    "print(df.head())   # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "df['processed_text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['encoded_label'], test_size=0.2, random_state=42)\n",
    " \n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Support Vector Machine (SVM) with TF-IDF\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Multinomial Naive Bayes with TF-IDF\n",
    "nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Random Forest Classifier with TF-IDF\n",
    "rf_model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Logistic Regression with TF-IDF\n",
    "lr_model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: Neural Network (MLP) with TF-IDF\n",
    "mlp_model = make_pipeline(TfidfVectorizer(), MLPClassifier(hidden_layer_sizes=(100,), max_iter=500))\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SVM): 0.7033333333333334\n",
      "Accuracy (Naive Bayes): 0.6516666666666666\n",
      "Accuracy (Random Forest): 0.6666666666666666\n",
      "Accuracy (Logistic Regression): 0.69\n",
      "Accuracy (Neural Network): 0.665\n"
     ]
    }
   ],
   "source": [
    "# Display the accuracies for each method\n",
    "print(\"Accuracy (SVM):\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Accuracy (Naive Bayes):\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Accuracy (Random Forest):\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Accuracy (Logistic Regression):\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Accuracy (Neural Network):\", accuracy_score(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Metrics (SVM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.41      0.51       113\n",
      "           1       0.60      0.05      0.10        55\n",
      "           2       0.40      0.04      0.07       106\n",
      "           3       0.45      0.09      0.15       144\n",
      "           4       0.72      0.99      0.83       782\n",
      "\n",
      "    accuracy                           0.70      1200\n",
      "   macro avg       0.57      0.32      0.33      1200\n",
      "weighted avg       0.65      0.70      0.62      1200\n",
      "\n",
      "\n",
      "Additional Metrics (Naive Bayes):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       113\n",
      "           1       0.00      0.00      0.00        55\n",
      "           2       0.00      0.00      0.00       106\n",
      "           3       0.00      0.00      0.00       144\n",
      "           4       0.65      1.00      0.79       782\n",
      "\n",
      "    accuracy                           0.65      1200\n",
      "   macro avg       0.13      0.20      0.16      1200\n",
      "weighted avg       0.42      0.65      0.51      1200\n",
      "\n",
      "\n",
      "Additional Metrics (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.12      0.20       113\n",
      "           1       1.00      0.04      0.07        55\n",
      "           2       0.00      0.00      0.00       106\n",
      "           3       1.00      0.02      0.04       144\n",
      "           4       0.66      1.00      0.80       782\n",
      "\n",
      "    accuracy                           0.67      1200\n",
      "   macro avg       0.70      0.23      0.22      1200\n",
      "weighted avg       0.67      0.67      0.55      1200\n",
      "\n",
      "\n",
      "Additional Metrics (Logistic Regression):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.27      0.38       113\n",
      "           1       0.67      0.04      0.07        55\n",
      "           2       0.60      0.03      0.05       106\n",
      "           3       0.49      0.12      0.20       144\n",
      "           4       0.70      0.99      0.82       782\n",
      "\n",
      "    accuracy                           0.69      1200\n",
      "   macro avg       0.62      0.29      0.30      1200\n",
      "weighted avg       0.66      0.69      0.60      1200\n",
      "\n",
      "\n",
      "Additional Metrics (Neural Network):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57       113\n",
      "           1       0.18      0.11      0.13        55\n",
      "           2       0.32      0.13      0.19       106\n",
      "           3       0.24      0.26      0.25       144\n",
      "           4       0.78      0.88      0.82       782\n",
      "\n",
      "    accuracy                           0.67      1200\n",
      "   macro avg       0.43      0.37      0.39      1200\n",
      "weighted avg       0.63      0.67      0.64      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# # Additional Metrics\n",
    "print(\"\\nAdditional Metrics (SVM):\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nAdditional Metrics (Naive Bayes):\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(\"\\nAdditional Metrics (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nAdditional Metrics (Logistic Regression):\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"\\nAdditional Metrics (Neural Network):\\n\", classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------- CHALLENGE CODE ENDS HERE --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "1. Assign the Accuracy score obtained for the model created in this challenge to the specified variable in the predefined function *submit_accuracy_score* below. The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
    "2. Please do not make any changes to the variable names and the function name *submit_accuracy_score* as this will be used for automated evaluation of the challenge. Any modification in these names will result in unexpected behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_accuracy_score()-> float:\n",
    "    #accuracy should be in the range of 0.0 to 1.0\n",
    "    accuracy = 0.0\n",
    "    # code starts here\n",
    "   \n",
    "    # code ends here\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
